# 快速上手开发第一个Flink应用程序

## 开发环境准备

```shell
λ echo %JAVA_HOME% 
D:\Program Files\Java\jdk1.8.0_251

λ echo %MAVEN_HOME%
D:\Applications\apache-maven-3.6.3
```

## 使用Flink开发一个批处理应用程序（Java/Scala）

### 需求：词频统计（Word count）

一个文件，统计文件中每个单词出现的次数

分隔符是 `\t`

统计结果直接打印到控制台（生产上市Sink到目的地）

 ### 实现：Flink + Java

#### 前置条件

The only requirements are working **Maven 3.0.4** (or higher) and **Java 8.x** installations.

#### 创建项目

Use one of the following commands to **create a project**:

- [Use **Maven archetypes**](https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/projectsetup/java_api_quickstart.html#maven-archetype)

  ```
      $ mvn archetype:generate                               \
        -DarchetypeGroupId=org.apache.flink              \
        -DarchetypeArtifactId=flink-quickstart-java      \
        -DarchetypeVersion=1.10.0
  ```

- [Run the **quickstart script**](https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/projectsetup/java_api_quickstart.html#quickstart-script)

  ```
      $ curl https://flink.apache.org/q/quickstart.sh | bash -s 1.10.0
  ```

#### 检查项目

项目创建后，工作目录将多出一个新目录。如果你使用的是 *curl* 方式创建项目，目录名为 `quickstart`； 如果你使用的是 *Maven archetypes* 方式创建项目，则目录名为你指定的 `artifactId`：

```
$ tree quickstart/
quickstart/
├── pom.xml
└── src
    └── main
        ├── java
        │   └── org
        │       └── myorg
        │           └── quickstart
        │               ├── BatchJob.java
        │               └── StreamingJob.java
        └── resources
            └── log4j.properties
```

示例项目是一个 **Maven project**，它包含了两个类：*StreamingJob* 和 *BatchJob* 分别是 *DataStream* and *DataSet* 程序的基础骨架程序。 *main* 方法是程序的入口，既可用于IDE测试/执行，也可用于部署。

我们建议你将 **此项目导入IDE** 来开发和测试它。 IntelliJ IDEA 支持 Maven 项目开箱即用。如果你使用的是 Eclipse，使用[m2e 插件](http://www.eclipse.org/m2e/) 可以 [导入 Maven 项目](http://books.sonatype.com/m2eclipse-book/reference/creating-sect-importing-projects.html#fig-creating-import)。 一些 Eclipse 捆绑包默认包含该插件，其他情况需要你手动安装。

> out of the box：OOTB开箱即用

*请注意*：对 Flink 来说，默认的 JVM 堆内存可能太小，你应当手动增加堆内存。 在 Eclipse 中，选择 `Run Configurations -> Arguments` 并在 `VM Arguments` 对应的输入框中写入：`-Xmx800m`。 在 IntelliJ IDEA 中，推荐从菜单 `Help | Edit Custom VM Options` 来修改 JVM 选项。有关详细信息，请参阅[这篇文章](https://intellij-support.jetbrains.com/hc/en-us/articles/206544869-Configuring-JVM-options-and-platform-properties)。

#### 构建项目

如果你想要 **构建/打包你的项目**，请在项目目录下运行 ‘`mvn clean package`’ 命令。 命令执行后，你将 **找到一个JAR文件**，里面包含了你的应用程序，以及已作为依赖项添加到应用程序的连接器和库：`target/<artifact-id>-<version>.jar`。

**注意：** 如果你使用其他类而不是 *StreamingJob* 作为应用程序的主类/入口， 我们建议你相应地修改 `pom.xml` 文件中的 `mainClass` 配置。这样， Flink 可以从 JAR 文件运行应用程序，而无需另外指定主类。

#### 下一步

开始编写应用！

如果你准备编写流处理应用，正在寻找灵感来写什么， 可以看看[流处理应用程序教程](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/getting-started/walkthroughs/datastream_api.html)

如果你准备编写批处理应用，正在寻找灵感来写什么， 可以看看[批处理应用程序示例](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/batch/examples.html)

有关 API 的完整概述，请查看 [DataStream API](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/datastream_api.html) 和 [DataSet API](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/batch/index.html) 章节。

在[这里](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/getting-started/tutorials/local_setup.html)，你可以找到如何在 IDE 之外的本地集群中运行应用程序。

#### 开发流程/开发八股文编程

1. `set up the batch execution environment`
2. `getting some data from the environment`
3. `transform operations` 开发的核心所在：开发业务逻辑
4. `execute program`

#### 功能拆解

	1. 读取数据
 	2. 每一行的数据按照指定的分隔符拆分
 	3. 为每一个单词赋上次数为1的
 	4. 合并操作

### 实现：Flink + Scala

#### 前置条件

The only requirements are working **Maven 3.0.4** (or higher) and **Java 8.x** installations.

#### 创建项目

Use one of the following commands to **create a project**:

- [Use **Maven archetypes**](https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/projectsetup/java_api_quickstart.html#maven-archetype)

  ```
      $ mvn archetype:generate                               \
        -DarchetypeGroupId=org.apache.flink              \
        -DarchetypeArtifactId=flink-quickstart-java      \
        -DarchetypeVersion=1.10.0
  ```

- [Run the **quickstart script**](https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/projectsetup/java_api_quickstart.html#quickstart-script)

  ```
      $ curl https://flink.apache.org/q/quickstart.sh | bash -s 1.10.0
  ```

Scala与Java基本类似，但是代码量简单很多

## 使用Flink开发一个实时处理应用程序（Java/Scala）

Java代码：

```java
public class StreamingWCJavaApp {

    public static void main(String[] args) throws Exception {

        // step1：获取执行环境
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // step2: 读取数据
        DataStreamSource<String> text = env.socketTextStream("localhost", 9999);

        // step3： transform
        text.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public void flatMap(String value, Collector<Tuple2<String, Integer>> collector) throws Exception {
                String[] tokens = value.toLowerCase().split(",");

                for (String token : tokens) {
                    if (token.length() > 0) {
                        collector.collect(new Tuple2<String, Integer>(token, 1));
                    }
                }

            }
        }).keyBy(0).timeWindow(Time.seconds(5)).sum(1).print().setParallelism(1);
        env.execute("StreamingWCJavaApp");

    }
}
```

需要注意的地方：

1. 大体上与批处理差不多，但是`env.execute("StreamingWCJavaApp");`最后记得要执行

2. 此次案例中有需要使用`nc -lp 9999`或`nc -lk 9999`将端口开启，注意该端口不能被占用

3. 代码可以重构，使用args参数传入变量，其中应该根据fromargs()的注释，使用`--key1 value1 --key2 value2 -key3 value3`的格式

   ```java
   	/**
   	 * Returns {@link ParameterTool} for the given arguments. The arguments are keys followed by values.
   	 * Keys have to start with '-' or '--'
   	 *
   	 * <p><strong>Example arguments:</strong>
   	 * --key1 value1 --key2 value2 -key3 value3
   	 *
   	 * @param args Input array arguments
   	 * @return A {@link ParameterTool}
   	 */
   ```

   

**Java重构后的代码:**

```java
public class StreamingWCJavaApp02 {

    public static void main(String[] args) throws Exception {

        // 获取参数
        int port;
        try {
            ParameterTool tool = ParameterTool.fromArgs(args);
            port = tool.getInt("port");
        }catch (Exception e){
            System.err.println("端口未设置，使用默认端口9999");
            port = 9998;
        }
        // step1：获取执行环境
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // step2: 读取数据
        DataStreamSource<String> text = env.socketTextStream("localhost", port);

        // step3： transform
        text.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public void flatMap(String value, Collector<Tuple2<String, Integer>> collector) throws Exception {
                String[] tokens = value.toLowerCase().split(",");

                for (String token : tokens) {
                    if (token.length() > 0) {
                        collector.collect(new Tuple2<String, Integer>(token, 1));
                    }
                }

            }
        }).keyBy(0).timeWindow(Time.seconds(5)).sum(1).print().setParallelism(1);

        env.execute("StreamingWCJavaApp02");

    }
```

## Flink Java vs Scala

1. 算子	map	filter
2. 简洁性

## 开发过程中依赖的注意事项

每个 Flink 应用都需要依赖一组 Flink 类库。Flink 应用至少需要依赖 Flink APIs。许多应用还会额外依赖连接器类库(比如 Kafka、Cassandra 等)。 当用户运行 Flink 应用时(无论是在 IDE 环境下进行测试，还是部署在分布式环境下)，运行时类库都必须可用。

### Flink 核心依赖以及应用依赖

与其他运行用户自定义应用的大多数系统一样，Flink 中有两大类依赖类库

- **Flink 核心依赖**：Flink 本身包含运行所需的一组类和依赖，比如协调、网络通讯、checkpoint、容错处理、API、算子(如窗口操作)、 资源管理等，这些类和依赖形成了 Flink 运行时的核心。当 Flink 应用启动时，这些依赖必须可用。

  这些核心类和依赖被打包在 `flink-dist` jar 里。它们是 Flink `lib` 文件夹下的一部分，也是 Flink 基本容器镜像的一部分。 这些依赖类似 Java `String` 和 `List` 的核心类库(`rt.jar`, `charsets.jar`等)。

  Flink 核心依赖不包含连接器和类库（如 CEP、SQL、ML 等），这样做的目的是默认情况下避免在类路径中具有过多的依赖项和类。 实际上，我们希望尽可能保持核心依赖足够精简，以保证一个较小的默认类路径，并且避免依赖冲突。

- **用户应用依赖**：是指特定的应用程序需要的类库，如连接器，formats等。

  用户应用代码和所需的连接器以及其他类库依赖通常被打包到 *application jar* 中。

  用户应用程序依赖项不需包括 Flink DataSet / DataStream API 以及运行时依赖项，因为它们已经是 Flink 核心依赖项的一部分。

### 搭建一个项目: 基础依赖

开发 Flink 应用程序需要最低限度的 API 依赖。Maven 用户，可以使用 [Java 项目模板](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/projectsetup/java_api_quickstart.html)或者 [Scala 项目模板](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/projectsetup/scala_api_quickstart.html)来创建一个包含最初依赖的程序骨架。

手动设置项目时，需要为 Java 或 Scala API 添加以下依赖项（这里以 Maven 语法为例，但也适用于其他构建工具（Gradle、 SBT 等））。

- [**Java**](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/projectsetup/dependencies.html#tab_java_0)

```xml
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
```

- [**Scala**](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/projectsetup/dependencies.html#tab_scala_0)

```xml
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-scala_2.11</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-scala_2.11</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
```

**注意事项:** 所有这些依赖项的作用域都应该设置为 *provided* 。 这意味着需要这些依赖进行编译，但不应将它们打包到项目生成的应用程序jar文件中– 因为这些依赖项是 Flink 的核心依赖，在应用启动前已经是可用的状态了。

我们强烈建议保持这些依赖的作用域为 *provided* 。 如果它们的作用域未设置为 *provided* ，则典型的情况是因为包含了 Flink 的核心依赖而导致生成的jar包变得过大。 最糟糕的情况是添加到应用程序的 Flink 核心依赖项与你自己的一些依赖项版本冲突（通常通过反向类加载来避免）。

**IntelliJ 上的一些注意事项:** 为了可以让 Flink 应用在 IntelliJ IDEA 中运行，这些 Flink 核心依赖的作用域需要设置为 *compile* 而不是 *provided* 。 否则 IntelliJ 不会添加这些依赖到 classpath，会导致应用运行时抛出 `NoClassDefFountError` 异常。为了避免声明这些依赖的作用域为 *compile* (因为我们不推荐这样做)， 上文给出的 Java 和 Scala 项目模板使用了一个小技巧：添加了一个 profile，仅当应用程序在 IntelliJ 中运行时该 profile 才会被激活， 然后将依赖作用域设置为 *compile* ，从而不影响应用 jar 包。

### 添加连接器以及类库依赖

大多数应用需要依赖特定的连接器或其他类库，例如 Kafka、Cassandra 的连接器等。这些连接器不是 Flink 核心依赖的一部分，因此必须作为依赖项手动添加到应用程序中。

下面是添加 Kafka 0.10 连接器依赖（Maven 语法）的示例：

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka-0.10_2.11</artifactId>
    <version>1.10.0</version>
</dependency>
```

我们建议将应用程序代码及其所有需要的依赖项打包到一个 *jar-with-dependencies* 的 jar 包中。 这个打包好的应用 jar 可以提交到已经运行的 Flink 集群中，或者添加到 Flink 应用容器镜像中。

通过[Java 项目模板](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/projectsetup/java_api_quickstart.html) 或者 [Scala 项目模板](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/projectsetup/scala_api_quickstart.html) 创建的应用， 当使用命令 `mvn clean package` 打包的时候会自动将应用依赖类库打包进应用 jar 包。 对于不是通过上面模板创建的应用，我们推荐添加 Maven Shade Plugin 去构建应用。(下面的附录会给出具体配置)

**注意:** 要使 Maven（以及其他构建工具）正确地将依赖项打包到应用程序 jar 中，必须将这些依赖项的作用域设置为 *compile* （与核心依赖项不同，后者作用域应该设置为 *provided* ）。

### Scala 版本

Scala 版本(2.10、2.11、2.12等)互相是不兼容的。因此，依赖 Scala 2.11 的 Flink 环境是不可以运行依赖 Scala 2.12 应用的。

所有依赖 Scala 的 Flink 类库都以它们依赖的 Scala 版本为后缀，例如 `flink-streaming-scala_2.11`。

只使用 Java 的开发人员可以选择任何 Scala 版本，Scala 开发人员需要选择与其应用程序相匹配的 Scala 版本。

对于指定的 Scala 版本如何构建 Flink 应用可以参考 [构建指南](https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/flinkDev/building.html#scala-versions)。