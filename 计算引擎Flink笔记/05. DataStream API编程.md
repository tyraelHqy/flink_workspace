# DataStream API编程

## DataStream API开发概述

DataStream programs in Flink are regular programs that implement transformations on **data streams** (e.g., **filtering, updating state, defining windows, aggregating**). The data streams are initially created from **various sources** (e.g., **message queues**, **socket streams**, **files**). Results are returned via **sinks**, which may for example write the data to files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs. The execution can happen in a local JVM, or on clusters of many machines.

In order to create your own Flink DataStream program, we encourage you to start with [anatomy of a Flink Program](https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/datastream_api.html#anatomy-of-a-flink-program) and gradually add your own [stream transformations](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/index.html). The remaining sections act as references for additional operations and advanced features.

## Data Sources

- [**Java**](https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/datastream_api.html#tab_java_2)

Sources are where your program reads its input from. You can attach a source to your program by using `StreamExecutionEnvironment.addSource(sourceFunction)`. Flink comes with a number of pre-implemented source functions, but you can always write your own custom sources by implementing the `SourceFunction` **for non-parallel sources**, or by implementing the `ParallelSourceFunction` interface or extending the `RichParallelSourceFunction` for parallel sources.

There are several predefined stream sources accessible from the `StreamExecutionEnvironment`:

File-based:

- `readTextFile(path)` - Reads text files, i.e. files that respect the `TextInputFormat` specification, line-by-line and returns them as Strings.

- `readFile(fileInputFormat, path)` - Reads (once) files as dictated by the specified file input format.

- `readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)` - This is the method called internally by the two previous ones. It reads files in the `path` based on the given `fileInputFormat`. Depending on the provided `watchType`, this source may periodically monitor (every `interval` ms) the path for new data (`FileProcessingMode.PROCESS_CONTINUOUSLY`), or process once the data currently in the path and exit (`FileProcessingMode.PROCESS_ONCE`). Using the `pathFilter`, the user can further exclude files from being processed.

  *IMPLEMENTATION:*

  Under the hood, Flink splits the file reading process into two sub-tasks, namely *directory monitoring* and *data reading*. Each of these sub-tasks is implemented by a separate entity. Monitoring is implemented by a single, **non-parallel** (parallelism = 1) task, while reading is performed by multiple tasks running in parallel. The parallelism of the latter is equal to the job parallelism. The role of the single monitoring task is to scan the directory (periodically or only once depending on the `watchType`), find the files to be processed, divide them in *splits*, and assign these splits to the downstream readers. The readers are the ones who will read the actual data. Each split is read by only one reader, while a reader can read multiple splits, one-by-one.

  *IMPORTANT NOTES:*

  1. If the `watchType` is set to `FileProcessingMode.PROCESS_CONTINUOUSLY`, when a file is modified, its contents are re-processed entirely. This can break the “exactly-once” semantics, as appending data at the end of a file will lead to **all** its contents being re-processed.
  2. If the `watchType` is set to `FileProcessingMode.PROCESS_ONCE`, the source scans the path **once** and exits, without waiting for the readers to finish reading the file contents. Of course the readers will continue reading until all file contents are read. Closing the source leads to no more checkpoints after that point. This may lead to slower recovery after a node failure, as the job will resume reading from the last checkpoint.

Socket-based:

- `socketTextStream` - Reads from a socket. Elements can be separated by a delimiter分隔符.

Collection-based:

- `fromCollection(Collection)` - Creates a data stream from the Java Java.util.Collection. All elements in the collection must be of the same type.
- `fromCollection(Iterator, Class)` - Creates a data stream from an iterator. The class specifies the data type of the elements returned by the iterator.
- `fromElements(T ...)` - Creates a data stream from the given sequence of objects. All objects must be of the same type.
- `fromParallelCollection(SplittableIterator, Class)` - Creates a data stream from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
- `generateSequence(from, to)` - Generates the sequence of numbers in the given interval, in parallel.

Custom:

- `addSource` - Attach a new source function. For example, to read from Apache Kafka you can use `addSource(new FlinkKafkaConsumer010<>(...))`. See [connectors](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/index.html) for more details.

- [**Scala**](https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/datastream_api.html#tab_scala_2)

Sources are where your program reads its input from. You can attach a source to your program by using `StreamExecutionEnvironment.addSource(sourceFunction)`. Flink comes with a number of pre-implemented source functions, but you can always write your own custom sources by implementing the `SourceFunction` for non-parallel sources, or by implementing the `ParallelSourceFunction` interface or extending the `RichParallelSourceFunction` for parallel sources.

There are several predefined stream sources accessible from the `StreamExecutionEnvironment`:

File-based:

- `readTextFile(path)` - Reads text files, i.e. files that respect the `TextInputFormat` specification, line-by-line and returns them as Strings.

- `readFile(fileInputFormat, path)` - Reads (once) files as dictated by the specified file input format.

- `readFile(fileInputFormat, path, watchType, interval, pathFilter)` - This is the method called internally by the two previous ones. It reads files in the `path` based on the given `fileInputFormat`. Depending on the provided `watchType`, this source may periodically monitor (every `interval` ms) the path for new data (`FileProcessingMode.PROCESS_CONTINUOUSLY`), or process once the data currently in the path and exit (`FileProcessingMode.PROCESS_ONCE`). Using the `pathFilter`, the user can further exclude files from being processed.

  *IMPLEMENTATION:*

  Under the hood, Flink splits the file reading process into two sub-tasks, namely *directory monitoring* and *data reading*. Each of these sub-tasks is implemented by a separate entity. Monitoring is implemented by a single, **non-parallel** (parallelism = 1) task, while reading is performed by multiple tasks running in parallel. The parallelism of the latter is equal to the job parallelism. The role of the single monitoring task is to scan the directory (periodically or only once depending on the `watchType`), find the files to be processed, divide them in *splits*, and assign these splits to the downstream readers. The readers are the ones who will read the actual data. Each split is read by only one reader, while a reader can read multiple splits, one-by-one.

  *IMPORTANT NOTES:*

  1. If the `watchType` is set to `FileProcessingMode.PROCESS_CONTINUOUSLY`, when a file is modified, its contents are re-processed entirely. This can break the “exactly-once” semantics, as appending data at the end of a file will lead to **all** its contents being re-processed.
  2. If the `watchType` is set to `FileProcessingMode.PROCESS_ONCE`, the source scans the path **once** and exits, without waiting for the readers to finish reading the file contents. Of course the readers will continue reading until all file contents are read. Closing the source leads to no more checkpoints after that point. This may lead to slower recovery after a node failure, as the job will resume reading from the last checkpoint.

Socket-based:

- `socketTextStream` - Reads from a socket. Elements can be separated by a delimiter.

```scala
object DataStreamSourceApp {

  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    socketFunction(env)

    env.execute("DataStreamSourceApp")
  }

  def socketFunction(env: StreamExecutionEnvironment): Unit = {
    val data = env.socketTextStream("localhost",9999)

    data.print()
  }
}
```

出来的数据如图所示，前面的序号为并行度，如果想不显示需要使用`data.print().setParallelism(1)`将并行度设置为1

![image-20200708103932683](../images/image-20200708103932683.png)

Collection-based:

- `fromCollection(Seq)` - Creates a data stream from the Java Java.util.Collection. All elements in the collection must be of the same type.
- `fromCollection(Iterator)` - Creates a data stream from an iterator. The class specifies the data type of the elements returned by the iterator.
- `fromElements(elements: _*)` - Creates a data stream from the given sequence of objects. All objects must be of the same type.
- `fromParallelCollection(SplittableIterator)` - Creates a data stream from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
- `generateSequence(from, to)` - Generates the sequence of numbers in the given interval, in parallel.

Custom:

- `addSource` - Attach a new source function. For example, to read from Apache Kafka you can use `addSource(new FlinkKafkaConsumer010<>(...))`. See [connectors](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/) for more details.

- Flink中使用数据源

  - implementing the `SourceFunction` for non-parallel sources

  ```scala
  class CustomNonParallelSourceFunction extends SourceFunction[Long] {
  
    var count = 1L
  
    var isRunning = true
  
    override def cancel(): Unit = {
      isRunning = false
    }
  
    override def run(ctx: SourceFunction.SourceContext[Long]): Unit = {
      while (isRunning) {
        ctx.collect(count)
  
        count += 1
  
        Thread.sleep(1000)
      }
    }
  }
  ```

  注意这个数据源是不能并行的，所以需要在调用该数据源的时候设置并行度为1

  ```scala
  val data = env.addSource(new CustomNonParallelSourceFunction).setParallelism(1)
  ```

  - implementing the `ParallelSourceFunction` interface
  - extending the `RichParallelSourceFunction` for parallel sources.

## DataStream Transformations

- Map

  DataStream → DataStream

  Takes one element and produces one element. A map function that doubles the values of the input stream:

  ```java
  DataStream<Integer> dataStream = //...
  dataStream.map(new MapFunction<Integer, Integer>() {
      @Override
      public Integer map(Integer value) throws Exception {
          return 2 * value;
      }
  });
  ```

- FlatMap

  DataStream → DataStream

  Takes one element and produces zero, one, or more elements. A flatmap function that splits sentences to words:

  ```java
  dataStream.flatMap(new FlatMapFunction<String, String>() {
      @Override
      public void flatMap(String value, Collector<String> out)
          throws Exception {
          for(String word: value.split(" ")){
              out.collect(word);
          }
      }
  });
  ```

- Filter

  DataStream → DataStream

  Evaluates a boolean function for each element and retains those for which the function returns true. A filter that filters out zero values:

  ```java
  dataStream.filter(new FilterFunction<Integer>() {
      @Override
      public boolean filter(Integer value) throws Exception {
          return value != 0;
      }
  });
  ```

- **KeyBy**
  DataStream → KeyedStream

  Logically partitions a stream into disjoint partitions. All records with the same key are assigned to the same partition. Internally, *keyBy()* is implemented with hash partitioning. There are different ways to [specify keys](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#keyed-datastream).

  This transformation returns a *KeyedStream*, which is, among other things, required to use [keyed state](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/state/state.html#keyed-state).

  ```java
  dataStream.keyBy("someKey") // Key by field "someKey"
  dataStream.keyBy(0) // Key by the first element of a Tuple
  ```

  **Attention** A type **cannot be a key** if:

  1. it is a POJO type but does not override the *hashCode()* method and relies on the *Object.hashCode()* implementation.
  2. it is an array of any type.

- **Reduce**
  KeyedStream → DataStream

  A "rolling" reduce on a keyed data stream. Combines the current element with the last reduced value and emits the new value.

  A reduce function that creates a stream of partial sums:

  ```java
  keyedStream.reduce(new ReduceFunction<Integer>() {
      @Override
      public Integer reduce(Integer value1, Integer value2)
      throws Exception {
          return value1 + value2;
      }
  });
  ```

- **Fold**
  KeyedStream → DataStream

  A "rolling" fold on a keyed data stream with an initial value. Combines the current element with the last folded value and emits the new value.

  A fold function that, when applied on the sequence (1,2,3,4,5), emits the sequence "start-1", "start-1-2", "start-1-2-3", ...

  ```java
  DataStream<String> result =
    keyedStream.fold("start", new FoldFunction<Integer, String>() {
      @Override
      public String fold(String current, Integer value) {
          return current + "-" + value;
      }
    });
  ```

- **Aggregations**
  KeyedStream → DataStream

  Rolling aggregations on a keyed data stream. The difference between min and minBy is that min returns the minimum value, whereas minBy returns the element that has the minimum value in this field (same for max and maxBy).

  ```java
  keyedStream.sum(0);
  keyedStream.sum("key");
  keyedStream.min(0);
  keyedStream.min("key");
  keyedStream.max(0);
  keyedStream.max("key");
  keyedStream.minBy(0);
  keyedStream.minBy("key");
  keyedStream.maxBy(0);
  keyedStream.maxBy("key");
  ```

- **Window**
  KeyedStream → WindowedStream

  Windows can be defined on already partitioned KeyedStreams. Windows group the data in each key according to some characteristic (e.g., the data that arrived within the last 5 seconds). See [windows](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html) for a complete description of windows.

  ```java
  dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data
  ```

- **WindowAll**
  DataStream → AllWindowedStream
  
  Windows can be defined on regular DataStreams. Windows group all the stream events according to some characteristic (e.g., the data that arrived within the last 5 seconds). See [windows](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/windows.html) for a complete description of windows.
  
  **WARNING:** This is in many cases a **non-parallel** transformation. All records will be gathered in one task for the windowAll operator.
  
  ```java
  dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data
  ```
  
- **Window Apply**
  WindowedStream → DataStream
  AllWindowedStream → DataStream

  Applies a general function to the window as a whole. Below is a function that manually sums the elements of a window.

  **Note:** If you are using a windowAll transformation, you need to use an AllWindowFunction instead.

  ```java
  windowedStream.apply (new WindowFunction<Tuple2<String,Integer>, Integer, Tuple, Window>() {
      public void apply (Tuple tuple,
              Window window,
              Iterable<Tuple2<String, Integer>> values,
              Collector<Integer> out) throws Exception {
          int sum = 0;
          for (value t: values) {
              sum += t.f1;
          }
          out.collect (new Integer(sum));
      }
  });
  
  // applying an AllWindowFunction on non-keyed window stream
  allWindowedStream.apply (new AllWindowFunction<Tuple2<String,Integer>, Integer, Window>() {
      public void apply (Window window,
              Iterable<Tuple2<String, Integer>> values,
              Collector<Integer> out) throws Exception {
          int sum = 0;
          for (value t: values) {
              sum += t.f1;
          }
          out.collect (new Integer(sum));
      }
  });
  ```

- **Window Reduce**
  WindowedStream → DataStream

  Applies a functional reduce function to the window and returns the reduced value.

  ```java
  windowedStream.reduce (new ReduceFunction<Tuple2<String,Integer>>() {
      public Tuple2<String, Integer> reduce(Tuple2<String, Integer> value1, Tuple2<String, Integer> value2) throws Exception {
          return new Tuple2<String,Integer>(value1.f0, value1.f1 + value2.f1);
      }
  });
  ```

- **Window Fold**
  WindowedStream → DataStream

  Applies a functional fold function to the window and returns the folded value. The example function, when applied on the sequence (1,2,3,4,5), folds the sequence into the string "start-1-2-3-4-5":

  ```java
  windowedStream.fold("start", new FoldFunction<Integer, String>() {
      public String fold(String current, Integer value) {
          return current + "-" + value;
      }
  });
  ```

- **Aggregations on windows**
  WindowedStream → DataStream

  Aggregates the contents of a window. The difference between min and minBy is that min returns the minimum value, whereas minBy returns the element that has the minimum value in this field (same for max and maxBy).

  ```java
  windowedStream.sum(0);
  windowedStream.sum("key");
  windowedStream.min(0);
  windowedStream.min("key");
  windowedStream.max(0);
  windowedStream.max("key");
  windowedStream.minBy(0);
  windowedStream.minBy("key");
  windowedStream.maxBy(0);
  windowedStream.maxBy("key");
  ```

- **Union**
  DataStream* → DataStream

  Union of two or more data streams creating a new stream containing all the elements from all the streams. Note: If you union a data stream with itself you will get each element twice in the resulting stream.

  ```java
  dataStream.union(otherStream1, otherStream2, ...);
  ```

- **Window Join**
  DataStream,DataStream → DataStream

  Join two data streams on a given key and a common window.

  ```java
  dataStream.join(otherStream)
      .where(<key selector>).equalTo(<key selector>)
      .window(TumblingEventTimeWindows.of(Time.seconds(3)))
      .apply (new JoinFunction () {...});
  ```

- **Interval Join**
  KeyedStream,KeyedStream → DataStream

  Join two elements e1 and e2 of two keyed streams with a common key over a given time interval, so that e1.timestamp + lowerBound <= e2.timestamp <= e1.timestamp + upperBound

  ```java
  // this will join the two streams so that
  // key1 == key2 && leftTs - 2 < rightTs < leftTs + 2
  keyedStream.intervalJoin(otherKeyedStream)
      .between(Time.milliseconds(-2), Time.milliseconds(2)) // lower and upper bound
      .upperBoundExclusive(true) // optional
      .lowerBoundExclusive(true) // optional
      .process(new IntervalJoinFunction() {...});
  ```

- **Window CoGroup**
  DataStream,DataStream → DataStream

  Cogroups two data streams on a given key and a common window.

  ```java
  dataStream.coGroup(otherStream)
      .where(0).equalTo(1)
      .window(TumblingEventTimeWindows.of(Time.seconds(3)))
      .apply (new CoGroupFunction () {...});
  ```

- **Connect**
  DataStream,DataStream → ConnectedStreams

  "Connects" two data streams retaining their types. Connect allowing for shared state between the two streams.

  ```java
  DataStream<Integer> someStream = //...
  DataStream<String> otherStream = //...
  
  ConnectedStreams<Integer, String> connectedStreams = someStream.connect(otherStream);
  ```

- **CoMap, CoFlatMap**
  ConnectedStreams → DataStream

  Similar to map and flatMap on a connected data stream

  ```java
  connectedStreams.map(new CoMapFunction<Integer, String, Boolean>() {
      @Override
      public Boolean map1(Integer value) {
          return true;
      }
  
      @Override
      public Boolean map2(String value) {
          return false;
      }
  });
  connectedStreams.flatMap(new CoFlatMapFunction<Integer, String, String>() {
  
     @Override
     public void flatMap1(Integer value, Collector<String> out) {
         out.collect(value.toString());
     }
  
     @Override
     public void flatMap2(String value, Collector<String> out) {
         for (String word: value.split(" ")) {
           out.collect(word);
         }
     }
  });
  ```

- **Split**
  DataStream → SplitStream

  Split the stream into two or more streams according to some criterion.

  ```java
  SplitStream<Integer> split = someDataStream.split(new OutputSelector<Integer>() {
      @Override
      public Iterable<String> select(Integer value) {
          List<String> output = new ArrayList<String>();
          if (value % 2 == 0) {
              output.add("even");
          }
          else {
              output.add("odd");
          }
          return output;
      }
  });
  ```

- **Select**
  SplitStream → DataStream

  Select one or more streams from a split stream.

  ```java
  SplitStream<Integer> split;
  DataStream<Integer> even = split.select("even");
  DataStream<Integer> odd = split.select("odd");
  DataStream<Integer> all = split.select("even","odd");
  ```

- **Iterate**
  DataStream → IterativeStream → DataStream

  Creates a "feedback" loop in the flow, by redirecting the output of one operator to some previous operator. This is especially useful for defining algorithms that continuously update a model. The following code starts with a stream and applies the iteration body continuously. Elements that are greater than 0 are sent back to the feedback channel, and the rest of the elements are forwarded downstream. See [iterations](https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/index.html#iterations) for a complete description.

  ```java
  IterativeStream<Long> iteration = initialStream.iterate();
  DataStream<Long> iterationBody = iteration.map (/*do something*/);
  DataStream<Long> feedback = iterationBody.filter(new FilterFunction<Long>(){
      @Override
      public boolean filter(Long value) throws Exception {
          return value > 0;
      }
  });
  iteration.closeWith(feedback);
  DataStream<Long> output = iterationBody.filter(new FilterFunction<Long>(){
      @Override
      public boolean filter(Long value) throws Exception {
          return value <= 0;
      }
  });
  ```

- **Project**
  DataStream → DataStream

  Selects a subset of fields from the tuples

  ```java
  DataStream<Tuple3<Integer, Double, String>> in = // [...]
  DataStream<Tuple2<String, Integer>> out = in.project(2,0);
  ```