# DataSet API编程

## DataSet API开发概述

DataSet programs in Flink are regular programs that implement transformations on data sets (e.g., **filtering, mapping, joining, grouping**). The data sets are initially created from certain sources (e.g., by **reading files,** or **from local collections**). Results are returned via sinks, which may for example write the data to (distributed) files, or to standard output (for example the command line terminal). Flink programs run in a variety of **contexts**, **standalone**, or **embedded in other programs**. The execution can happen in a local JVM, or on clusters of many machines.

Source：源头**reading files,** or **from local collections**

​	Source ==》 flink（transformations） ==》 Sink

Sink：目的地**filtering, mapping, joining, grouping**

## DataSource

Data sources create the initial data sets, such as from files or from Java collections. The general mechanism of creating data sets is abstracted behind an **[InputFormat](https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/common/io/InputFormat.java)**. Flink comes with several built-in formats to create data sets from common file formats. Many of them have shortcut methods on the *ExecutionEnvironment*.

- 基于文件File-based:

  - `readTextFile(path)` / `TextInputFormat` - Reads files line wise and returns them as Strings.
  - `readTextFileWithValue(path)` / `TextValueInputFormat` - Reads files line wise and returns them as StringValues. StringValues are mutable strings.
  - `readCsvFile(path)` / `CsvInputFormat` - Parses files of comma (or another char) delimited fields. Returns a DataSet of tuples or POJOs. Supports the basic java types and their Value counterparts as field types.
  - `readFileOfPrimitives(path, Class)` / `PrimitiveInputFormat` - Parses files of new-line (or another char sequence) delimited primitive data types such as `String` or `Integer`.
  - `readFileOfPrimitives(path, delimiter, Class)` / `PrimitiveInputFormat` - Parses files of new-line (or another char sequence) delimited primitive data types such as `String` or `Integer` using the given delimiter.

- 基于集合Collection-based:

  - `fromCollection(Collection)` - Creates a data set from a Java.util.Collection. All elements in the collection must be of the same type.
  - `fromCollection(Iterator, Class)` - Creates a data set from an iterator. The class specifies the data type of the elements returned by the iterator.
  - `fromElements(T ...)` - Creates a data set from the given sequence of objects. All objects must be of the same type.
  - `fromParallelCollection(SplittableIterator, Class)` - Creates a data set from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
  - `generateSequence(from, to)` - Generates the sequence of numbers in the given interval, in parallel.

- #### 基于CSV（Configuring CSV Parsing）

  Flink offers a number of configuration options for CSV parsing:

  - `types(Class ... types)` specifies the types of the fields to parse. **It is mandatory to configure the types of the parsed fields.** In case of the type class Boolean.class, “True” (case-insensitive), “False” (case-insensitive), “1” and “0” are treated as booleans.
  - `lineDelimiter(String del)` specifies the delimiter of individual records. The default line delimiter is the new-line character `'\n'`.
  - `fieldDelimiter(String del)` specifies the delimiter that separates fields of a record. The default field delimiter is the comma character `','`.
  - `includeFields(boolean ... flag)`, `includeFields(String mask)`, or `includeFields(long bitMask)` defines which fields to read from the input file (and which to ignore). By default the first *n* fields (as defined by the number of types in the `types()` call) are parsed.
  - `parseQuotedStrings(char quoteChar)` enables quoted string parsing. Strings are parsed as quoted strings if the first character of the string field is the quote character (leading or tailing whitespaces are *not* trimmed). Field delimiters within quoted strings are ignored. Quoted string parsing fails if the last character of a quoted string field is not the quote character or if the quote character appears at some point which is not the start or the end of the quoted string field (unless the quote character is escaped using ‘'). If quoted string parsing is enabled and the first character of the field is *not* the quoting string, the string is parsed as unquoted string. By default, quoted string parsing is disabled.
  - `ignoreComments(String commentPrefix)` specifies a comment prefix. All lines that start with the specified comment prefix are not parsed and ignored. By default, no lines are ignored.
  - `ignoreInvalidLines()` enables lenient parsing, i.e., lines that cannot be correctly parsed are ignored. By default, lenient parsing is disabled and invalid lines raise an exception.
  - `ignoreFirstLine()` configures the InputFormat to ignore the first line of the input file. By default no line is ignored.

- #### 从递归文件夹的内容创建DataSet（Recursive Traversal of the Input Path Directory）

  For file-based inputs, when the input path is a directory, nested files are not enumerated by default. Instead, only the files inside the base directory are read, while nested files are ignored. Recursive enumeration of nested files can be enabled through the `recursive.file.enumeration` configuration parameter, like in the following example.

  ```java
  // enable recursive enumeration of nested input files
  ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
  
  // create a configuration object
  Configuration parameters = new Configuration();
  
  // set the recursive enumeration parameter
  parameters.setBoolean("recursive.file.enumeration", true);
  
  // pass the configuration to the data source
  DataSet<String> logs = env.readTextFile("file:///path/with.nested/files")
  			  .withParameters(parameters);
  ```

- ### 读取压缩文件(Read Compressed Files)

  Flink currently supports transparent decompression of input files if these are marked with an appropriate file extension. In particular, this means that no further configuration of the input formats is necessary and any `FileInputFormat` support the compression, including custom input formats. Please notice that compressed files might not be read in parallel, thus impacting job scalability.

  The following table lists the currently supported compression methods.

| Compression method | File extensions | Parallelizable |
| :----------------- | :-------------- | :------------- |
| **DEFLATE**        | `.deflate`      | no             |
| **GZip**           | `.gz`, `.gzip`  | no             |
| **Bzip2**          | `.bz2`          | no             |
| **XZ**             | `.xz`           | no             |

## Sink

## 计数器

## 分布式缓存





